{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import structural_similarity_index_measure\n",
    "\n",
    "def ssim(cam1, cam2):\n",
    "    \"\"\"\n",
    "    Compute the ssim similarity between two saliency maps.\n",
    "    \"\"\"\n",
    "    # cam1 = cam1.flatten()\n",
    "    # cam2 = cam2.flatten()\n",
    "    # return sum(abs(cam1 - cam2))\n",
    "    # compute ssmi\n",
    "    cam1 = torch.tensor(cam1).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "    cam2 = torch.tensor(cam2).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "    ssim_score = structural_similarity_index_measure(cam1, cam2, data_range=1.0)\n",
    "    return ssim_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transform for CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # standard CIFAR-10 normalization\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Task label splits\n",
    "tasks = [\n",
    "    [0, 1, 2],   # Task 1\n",
    "    [3, 4, 5],   # Task 2\n",
    "    [6, 7, 8]    # Task 3\n",
    "]\n",
    "\n",
    "def filter_dataset_by_labels(dataset, labels):\n",
    "    \"\"\"Return indices of dataset samples having the specified labels.\"\"\"\n",
    "    indices = [i for i, (_, label) in enumerate(dataset) if label in labels]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Create subsets for each task\n",
    "train_subsets = [filter_dataset_by_labels(train_dataset, task) for task in tasks]\n",
    "test_subsets = [filter_dataset_by_labels(test_dataset,  task) for task in tasks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVGG(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        \"\"\"\n",
    "        num_classes will be set to 5 for tasks [0–4] and 5 for tasks [5–9]\n",
    "        in this example. Adjust if you want a single head with 10 classes or a multi-head setup.\n",
    "        \"\"\"\n",
    "        super(SimpleVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(8 * 8 * 128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=200):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer_data = []\n",
    "        self.buffer_labels = []\n",
    "\n",
    "    def add_samples(self, data, labels):\n",
    "        # If buffer is full, remove random samples to make space\n",
    "        if len(self.buffer_data) >= self.buffer_size:\n",
    "            # We can do random replacement or FIFO. Here, random removal for illustration.\n",
    "            to_remove = len(self.buffer_data) + len(data) - self.buffer_size\n",
    "            indices_to_remove = random.sample(range(len(self.buffer_data)), to_remove)\n",
    "            for idx in sorted(indices_to_remove, reverse=True):\n",
    "                del self.buffer_data[idx]\n",
    "                del self.buffer_labels[idx]\n",
    "\n",
    "        self.buffer_data.extend(data)\n",
    "        self.buffer_labels.extend(labels)\n",
    "\n",
    "    def get_samples(self, batch_size):\n",
    "        \"\"\"Return a random batch from the replay buffer.\"\"\"\n",
    "        if len(self.buffer_data) == 0:\n",
    "            return None, None\n",
    "        indices = random.sample(range(len(self.buffer_data)), min(batch_size, len(self.buffer_data)))\n",
    "        replay_data = [self.buffer_data[i] for i in indices]\n",
    "        replay_labels = [self.buffer_labels[i] for i in indices]\n",
    "        return torch.stack(replay_data), torch.tensor(replay_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sequential(model, train_subsets, test_subsets, tasks, buffer_size=200,\n",
    "                     num_epochs=5, batch_size=64, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train model sequentially on each task with a replay buffer.\n",
    "    tasks: List of label subsets (e.g., [[0,1,2],[3,4,5],[6,7,8]])\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # To record metrics across tasks\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "\n",
    "    for task_idx, (train_subset, test_subset, labels) in enumerate(zip(train_subsets, test_subsets, tasks)):\n",
    "        print(f\"=== Training on Task {task_idx+1} with labels {labels} ===\")\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "        train = True\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(f\"task_{'>'.join(map(str, range(1, task_idx+2)))}_nr_epochs_{num_epochs}_buffersize_{buffer_size}.pth\"))\n",
    "            train = False\n",
    "            print(\"Model loaded from previous training instead of training from scratch\")\n",
    "        except:\n",
    "            print(\"No model found, training from scratch\")\n",
    "            pass\n",
    "        if train:\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                for images, lbls in train_loader:\n",
    "                    images, lbls = images.to(device), lbls.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, lbls)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    correct += predicted.eq(lbls).sum().item()\n",
    "                    total += lbls.size(0)\n",
    "\n",
    "                    # Experience Replay step\n",
    "                    # Mix replay buffer samples into training\n",
    "                    replay_images, replay_labels = replay_buffer.get_samples(batch_size//2)\n",
    "                    if replay_images is not None:\n",
    "                        replay_images = replay_images.to(device)\n",
    "                        replay_labels = replay_labels.to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        replay_outputs = model(replay_images)\n",
    "                        replay_loss = criterion(replay_outputs, replay_labels)\n",
    "                        replay_loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                epoch_loss = running_loss / len(train_loader)\n",
    "                epoch_acc = 100. * correct / total\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%\")\n",
    "            model.save(f\"task_{'>'.join(map(str, range(1, task_idx+2)))}_nr_epochs_{num_epochs}_buffersize_{buffer_size}.pth\")\n",
    "\n",
    "        # Evaluate on the current task\n",
    "        task_train_acc, train_correct, train_total = evaluate_accuracy(model, train_loader, device)\n",
    "        task_test_acc , test_correct, test_total = evaluate_accuracy(model, test_loader, device)\n",
    "        \n",
    "\n",
    "        train_acc_history.append(task_train_acc)\n",
    "        test_acc_history.append(task_test_acc)\n",
    "\n",
    "        print(f\"Task {task_idx+1} Train Accuracy: {task_train_acc:.2f}% ({train_correct}/{train_total})\")\n",
    "        print(f\"Task {task_idx+1} Test Accuracy:  {task_test_acc:.2f}% ({test_correct}/{test_total})\\n\")\n",
    "\n",
    "        for task_idx_test, test_subset in enumerate(test_subsets):\n",
    "            test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "            test_acc, test_correct, test_total = evaluate_accuracy(model, test_loader, device)\n",
    "            nr_incorrect = test_total - test_correct\n",
    "            print(f\"Task {task_idx+1} -> Test Accuracy on {task_idx_test+1} : {test_acc:.2f}% ({test_correct}/{test_total}), Incorrect: {nr_incorrect}\")\n",
    "\n",
    "\n",
    "        # Add random samples from the current task to replay buffer\n",
    "        store_in_replay_buffer(model, train_subset, replay_buffer, device, store_size=buffer_size//3)\n",
    "\n",
    "    return model, train_acc_history, test_acc_history\n",
    "\n",
    "\n",
    "def store_in_replay_buffer(model, dataset_subset, replay_buffer, device, store_size=50):\n",
    "    \"\"\"\n",
    "    Randomly select store_size samples from the dataset_subset and add to replay buffer.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(dataset_subset), size=store_size, replace=False)\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx in indices:\n",
    "        x, y = dataset_subset[idx]\n",
    "        data.append(x)\n",
    "        labels.append(y)\n",
    "    replay_buffer.add_samples(data, labels)\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100.0 * correct / total, correct, total\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Minimal Grad-CAM implementation for a SimpleVGG model.\n",
    "    target_layer_name = 'features.7' by default (the last conv layer).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_name='features.7'):\n",
    "        self.model = model\n",
    "        self.target_layer_name = target_layer_name\n",
    "\n",
    "        # Will be set by hooks\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # Register hooks\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _save_gradient(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        \"\"\"\n",
    "        This finds the target layer by name and registers the forward/backward hooks\n",
    "        to capture the activations and gradients.\n",
    "        \"\"\"\n",
    "        # Build a dictionary of all modules by name\n",
    "        modules_dict = dict([*self.model.named_modules()])\n",
    "\n",
    "        # Confirm your layer name is in the dictionary (e.g., 'features.7')\n",
    "        if self.target_layer_name not in modules_dict:\n",
    "            raise ValueError(f\"Layer {self.target_layer_name} not found in model. \"\n",
    "                             f\"Available layers: {list(modules_dict.keys())}\")\n",
    "\n",
    "        target_layer = modules_dict[self.target_layer_name]\n",
    "\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # grad_out is a tuple with one element for the output gradient\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        # Register forward and backward hooks\n",
    "        target_layer.register_forward_hook(forward_hook)\n",
    "        target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        input_tensor: shape [B, 3, H, W]\n",
    "        target_class: integer index of class to target. If None, uses predicted class.\n",
    "        Returns: A list of CAM heatmaps (one per sample in the batch).\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        self.model.zero_grad()  # Clear any existing gradients\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # If target_class is None, use the top predicted class for each sample\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1)\n",
    "\n",
    "        # Convert target_class to a list if it's a single tensor\n",
    "        if isinstance(target_class, torch.Tensor):\n",
    "            target_class = target_class.cpu().tolist()  # e.g., [class_idx for each sample]\n",
    "\n",
    "        cams = []\n",
    "        batch_size = input_tensor.size(0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Backprop for sample i, class target_class[i]\n",
    "            self.model.zero_grad()  # zero grads for each sample\n",
    "            class_idx = target_class[i]\n",
    "            score = output[i, class_idx]  # scalar\n",
    "            score.backward(retain_graph=True)\n",
    "\n",
    "            # Get gradients & activations for sample i\n",
    "            gradients = self.gradients[i]       # shape: [128, 8, 8]\n",
    "            activations = self.activations[i]   # shape: [128, 8, 8]\n",
    "\n",
    "            # Compute channel-wise mean of gradients\n",
    "            alpha = gradients.mean(dim=(1, 2), keepdim=True)  # shape: [128, 1, 1]\n",
    "\n",
    "            # Linear combination of activations and alpha\n",
    "            cam = (activations * alpha).sum(dim=0)  # shape: [8, 8]\n",
    "            cam = F.relu(cam)  # ReLU to keep only positive activations\n",
    "\n",
    "            # Normalize to [0, 1]\n",
    "            cam -= cam.min()\n",
    "            if cam.max() != 0:\n",
    "                cam /= cam.max()\n",
    "\n",
    "            cams.append(cam.detach().cpu().numpy())\n",
    "\n",
    "        return cams\n",
    "\n",
    "\n",
    "def _compute_total_divergence(model_task1, model_task3, dataset, filter_fn=None):\n",
    "    \"\"\"\n",
    "    Compute the total L1 and L2 divergence across all misclassified samples.\n",
    "    if filter_fn is passed, only compute divergence for samples that pass the filter(y_pred_task1, y_pred_task3, y_true).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_task1.to(device)\n",
    "    model_task3.to(device)\n",
    "    model_task1.eval()\n",
    "    model_task3.eval()\n",
    "\n",
    "    gradcam_task1 = GradCAM(model_task1)\n",
    "    gradcam_task3 = GradCAM(model_task3)\n",
    "\n",
    "    total_ssim_score = 0\n",
    "    num_samples = 0\n",
    "\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        x, y_true = dataset[i]\n",
    "        x_ = x.unsqueeze(0).to(device)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred_task1 = model_task1(x_).argmax(dim=1).item()\n",
    "        y_pred_task3 = model_task3(x_).argmax(dim=1).item()\n",
    "\n",
    "        if filter_fn is None or filter_fn(y_pred_task1, y_pred_task3, y_true):\n",
    "            # Generate CAM for the ground-truth or the predicted class\n",
    "            cam_task1 = gradcam_task1.generate_cam(x_, target_class=None)[0]\n",
    "            cam_task3 = gradcam_task3.generate_cam(x_, target_class=None)[0]\n",
    "\n",
    "            # Compute similarity scores\n",
    "            ssim_score = ssim(cam_task1, cam_task3)\n",
    "\n",
    "            total_ssim_score += ssim_score\n",
    "            num_samples += 1\n",
    "\n",
    "    return total_ssim_score, num_samples\n",
    "\n",
    "def compute_total_divergence_misclassified(model_task1, model_task3, dataset):\n",
    "    return _compute_total_divergence(model_task1, model_task3, dataset, \n",
    "                                     filter_fn=lambda y_pred_task1, y_pred_task3, y_true: y_pred_task1 == y_true and y_pred_task3 != y_true)\n",
    "\n",
    "def compute_total_divergence_correctly_classified(model_task1, model_task3, dataset):\n",
    "    return _compute_total_divergence(model_task1, model_task3, dataset, \n",
    "                                     filter_fn=lambda y_pred_task1, y_pred_task3, y_true: y_pred_task1 == y_true and y_pred_task3 == y_true)\n",
    "\n",
    "\n",
    "def visualize_misclassified_samples(model_task1, model_task3, dataset, num_samples=20, output_folder=\"deliverables\"):\n",
    "    \"\"\"\n",
    "    1. Find samples in 'dataset' that model_task1 classified correctly but model_task3 misclassified.\n",
    "    2. Generate Grad-CAM for both models on those samples.\n",
    "    3. Plot side-by-side.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_task1.to(device)\n",
    "    model_task3.to(device)\n",
    "    model_task1.eval()\n",
    "    model_task3.eval()\n",
    "\n",
    "    gradcam_task1 = GradCAM(model_task1)\n",
    "    gradcam_task3 = GradCAM(model_task3)\n",
    "\n",
    "    misclassified_indices = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, y_true = dataset[i]\n",
    "            x_ = x.unsqueeze(0).to(device)\n",
    "\n",
    "            # Predictions\n",
    "            y_pred_task1 = model_task1(x_).argmax(dim=1).item()\n",
    "            y_pred_task3 = model_task3(x_).argmax(dim=1).item()\n",
    "\n",
    "            if y_pred_task1 == y_true and y_pred_task3 != y_true:\n",
    "                misclassified_indices.append(i)\n",
    "            if len(misclassified_indices) >= num_samples:\n",
    "                break\n",
    "\n",
    "    # Visualize\n",
    "    for idx in misclassified_indices:\n",
    "        x, y_true = dataset[idx]\n",
    "        x_ = x.unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate CAM for the ground-truth or the predicted class\n",
    "        cam_task1 = gradcam_task1.generate_cam(x_, target_class=None)[0]\n",
    "        cam_task3 = gradcam_task3.generate_cam(x_, target_class=None)[0]\n",
    "\n",
    "        # Plot\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "        # SSIM score\n",
    "        ssim_score = ssim(cam_task1, cam_task3)\n",
    "        fig.suptitle(f\"SSIM Score: {ssim_score:.2f}\")\n",
    "        # Original image\n",
    "        axs[0].imshow(unnormalize_and_convert_to_numpy(x))\n",
    "        axs[0].set_title(f\"Original (label={y_true})\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        # Grad-CAM from Task 1 model\n",
    "        axs[1].imshow(unnormalize_and_convert_to_numpy(x))\n",
    "        axs[1].imshow(cam_task1, cmap='jet', alpha=0.5)\n",
    "        axs[1].set_title(f\"Grad-CAM Task 1 (pred={model_task1(x_.to(device)).argmax(dim=1).item()})\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        # Grad-CAM from Task 3 model\n",
    "        axs[2].imshow(unnormalize_and_convert_to_numpy(x))\n",
    "        axs[2].imshow(cam_task3, cmap='jet', alpha=0.5)\n",
    "        axs[2].set_title(f\"Grad-CAM Task 3 (pred={model_task3(x_.to(device)).argmax(dim=1).item()})\")\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        #plt.show()\n",
    "        plt.savefig(f\"{output_folder}/misclassified_samples{idx}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "import random\n",
    "\n",
    "def visualize_correctly_classified_samples(model_task1, model_task3, dataset, num_samples=20, output_folder=\"deliverables\"):\n",
    "    \"\"\"\n",
    "    1. Find samples in 'dataset' that both model_task1 and model_task3 classified correctly.\n",
    "    2. Randomly select 'num_samples' from these correctly classified samples.\n",
    "    3. Generate Grad-CAM for both models on those samples.\n",
    "    4. Plot side-by-side.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_task1.to(device)\n",
    "    model_task3.to(device)\n",
    "    model_task1.eval()\n",
    "    model_task3.eval()\n",
    "\n",
    "    gradcam_task1 = GradCAM(model_task1)\n",
    "    gradcam_task3 = GradCAM(model_task3)\n",
    "\n",
    "    correctly_classified_indices = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, y_true = dataset[i]\n",
    "            x_ = x.unsqueeze(0).to(device)\n",
    "\n",
    "            # Predictions\n",
    "            y_pred_task1 = model_task1(x_).argmax(dim=1).item()\n",
    "            y_pred_task3 = model_task3(x_).argmax(dim=1).item()\n",
    "\n",
    "            if y_pred_task1 == y_true and y_pred_task3 == y_true:\n",
    "                correctly_classified_indices.append(i)\n",
    "\n",
    "    # Randomly select 'num_samples' from correctly classified samples\n",
    "    selected_indices = random.sample(correctly_classified_indices, min(num_samples, len(correctly_classified_indices)))\n",
    "\n",
    "    # Visualize\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        x, y_true = dataset[idx]\n",
    "        x_ = x.unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate CAM for the ground-truth or the predicted class\n",
    "        cam_task1 = gradcam_task1.generate_cam(x_, target_class=None)[0]\n",
    "        cam_task3 = gradcam_task3.generate_cam(x_, target_class=None)[0]\n",
    "\n",
    "        # Plot\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "        # compute l1 and l2 divergence\n",
    "        ssim_score = ssim(cam_task1, cam_task3)\n",
    "        fig.suptitle(f\"SSIM: {ssim_score:.2f}\")\n",
    "        # Original image\n",
    "        axs[0].imshow(unnormalize_and_convert_to_numpy(x))\n",
    "        axs[0].set_title(f\"Original (label={y_true})\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        # Grad-CAM from Task 1 model\n",
    "        axs[1].imshow(unnormalize_and_convert_to_numpy(x))\n",
    "        axs[1].imshow(cam_task1, cmap='jet', alpha=0.5)\n",
    "        axs[1].set_title(f\"Grad-CAM Task 1 (pred={model_task1(x_.to(device)).argmax(dim=1).item()})\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        # Grad-CAM from Task 3 model\n",
    "        axs[2].imshow(unnormalize_and_convert_to_numpy(x))\n",
    "        axs[2].imshow(cam_task3, cmap='jet', alpha=0.5)\n",
    "        axs[2].set_title(f\"Grad-CAM Task 3 (pred={model_task3(x_.to(device)).argmax(dim=1).item()})\")\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        #plt.show()\n",
    "        plt.savefig(f\"{output_folder}/correctly_classified_samples{i}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "def unnormalize_and_convert_to_numpy(tensor, mean=(0.4914, 0.4822, 0.4465),\n",
    "                                     std=(0.2470, 0.2435, 0.2616)):\n",
    "    # Convert from normalized tensor [C,H,W] to numpy array [H,W,C] for plotting\n",
    "    img = tensor.cpu().numpy().transpose((1, 2, 0))\n",
    "    img = img * np.array(std) + np.array(mean)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training on Task 1 with labels [0, 1, 2] ===\n",
      "Model loaded from previous training instead of training from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22389/1332495574.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"task_{'>'.join(map(str, range(1, task_idx+2)))}_nr_epochs_{num_epochs}_buffersize_{buffer_size}.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Train Accuracy: 98.73% (14810/15000)\n",
      "Task 1 Test Accuracy:  92.57% (2777/3000)\n",
      "\n",
      "Task 1 -> Test Accuracy on 1 : 92.57% (2777/3000), Incorrect: 223\n",
      "Task 1 -> Test Accuracy on 2 : 0.00% (0/3000), Incorrect: 3000\n",
      "Task 1 -> Test Accuracy on 3 : 0.00% (0/3000), Incorrect: 3000\n",
      "=== Training on Task 2 with labels [3, 4, 5] ===\n",
      "Model loaded from previous training instead of training from scratch\n",
      "Task 2 Train Accuracy: 97.44% (14616/15000)\n",
      "Task 2 Test Accuracy:  69.97% (2099/3000)\n",
      "\n",
      "Task 2 -> Test Accuracy on 1 : 50.23% (1507/3000), Incorrect: 1493\n",
      "Task 2 -> Test Accuracy on 2 : 69.97% (2099/3000), Incorrect: 901\n",
      "Task 2 -> Test Accuracy on 3 : 0.00% (0/3000), Incorrect: 3000\n",
      "=== Training on Task 3 with labels [6, 7, 8] ===\n",
      "Model loaded from previous training instead of training from scratch\n",
      "Task 3 Train Accuracy: 97.73% (14659/15000)\n",
      "Task 3 Test Accuracy:  88.50% (2655/3000)\n",
      "\n",
      "Task 3 -> Test Accuracy on 1 : 43.30% (1299/3000), Incorrect: 1701\n",
      "Task 3 -> Test Accuracy on 2 : 27.17% (815/3000), Incorrect: 2185\n",
      "Task 3 -> Test Accuracy on 3 : 88.50% (2655/3000), Incorrect: 345\n",
      "=== Training on Task 1 with labels [0, 1, 2] ===\n",
      "Model loaded from previous training instead of training from scratch\n",
      "Task 1 Train Accuracy: 98.45% (14768/15000)\n",
      "Task 1 Test Accuracy:  91.23% (2737/3000)\n",
      "\n",
      "Task 1 -> Test Accuracy on 1 : 91.23% (2737/3000), Incorrect: 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/repos/ethz-deep-learning-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "/home/alex/Documents/repos/ethz-deep-learning-2024/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracies across tasks: [98.73333333333333, 97.44, 97.72666666666667]\n",
      "Test accuracies across tasks: [92.56666666666666, 69.96666666666667, 88.5]\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Prepare data subsets\n",
    "# train_subsets, test_subsets already constructed above.\n",
    "\n",
    "# Initialize model\n",
    "#model = get_vgg_model(num_classes=10)\n",
    "model = SimpleVGG(num_classes=10)\n",
    "\n",
    "\n",
    "# Train the model sequentially on tasks\n",
    "model, train_acc, test_acc = train_sequential(\n",
    "    model, train_subsets, test_subsets, tasks,\n",
    "    buffer_size=3_000, num_epochs=8, batch_size=64, lr=0.001\n",
    ")\n",
    "\n",
    "# (Optional) Save a copy of the model after Task 1 for visualization\n",
    "# For demonstration, let's re-initialize and re-train only for tasks 1 (to compare).\n",
    "# Alternatively, you can store a checkpoint after finishing Task 1 training.\n",
    "model_task1 = SimpleVGG(num_classes=10)\n",
    "model_task1, _, _ = train_sequential(\n",
    "    model_task1, [train_subsets[0]], [test_subsets[0]], [tasks[0]],\n",
    "    buffer_size=0, num_epochs=8, batch_size=64, lr=0.001\n",
    ")\n",
    "\n",
    "# After finishing Task 3 in the main model, compare Grad-CAM on samples\n",
    "# that were correct for Task 1 model but are misclassified by the final model.\n",
    "visualize_misclassified_samples(\n",
    "    model_task1, model, test_subsets[0], num_samples=20\n",
    ")\n",
    "\n",
    "# Print or plot final accuracies\n",
    "print(\"Train accuracies across tasks:\", train_acc)\n",
    "print(\"Test accuracies across tasks:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_correctly_classified_samples(model_task1, model, test_subsets[0] + test_subsets[1] + test_subsets[2] , num_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassified_samples(\n",
    "    model_task1, model, test_subsets[0], num_samples=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM Score (Misclassified): 0.02\n",
      "Number of Misclassified Samples: 2488\n",
      "Average SSIM Score (Correctly Classified): 0.01\n",
      "Number of Correctly Classified Samples: 249\n"
     ]
    }
   ],
   "source": [
    "# Compute total divergences for misclassified samples\n",
    "ssim_score_misclassified, num_misclassified_samples = compute_total_divergence_misclassified(\n",
    "    model_task1, model, test_subsets[0]\n",
    ")\n",
    "\n",
    "# Compute total divergences for correctly classified samples\n",
    "ssim_score_correct, num_correct_samples = compute_total_divergence_correctly_classified(\n",
    "    model_task1, model, test_subsets[0] + test_subsets[1] + test_subsets[2]\n",
    ")\n",
    "\n",
    "print(f\"Average SSIM Score (Misclassified): {ssim_score_misclassified/num_misclassified_samples:.2f}\")\n",
    "print(f\"Number of Misclassified Samples: {num_misclassified_samples}\")\n",
    "\n",
    "print(f\"Average SSIM Score (Correctly Classified): {ssim_score_correct/num_correct_samples:.2f}\")\n",
    "print(f\"Number of Correctly Classified Samples: {num_correct_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, task_idx, num_epochs, buffer_size):\n",
    "    model.load_state_dict(torch.load(f\"task_{'>'.join(map(str, range(1, task_idx+2)))}_nr_epochs_{num_epochs}_buffersize_{buffer_size}.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22389/1272262104.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"task_{'>'.join(map(str, range(1, task_idx+2)))}_nr_epochs_{num_epochs}_buffersize_{buffer_size}.pth\"))\n",
      "/home/alex/Documents/repos/ethz-deep-learning-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the following we will compute the SSIM score between the model after each task T and the final model after task 3. The inputs used will correspond to the test set of task T.\n",
      "Buffer Size: 3000\n",
      "Model after Task 1 vs Model 1 evaluated on 1: No forgotten samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/repos/ethz-deep-learning-2024/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after Task 1 vs Model 1 evaluated on 1: Average SSIM Score (Correctly Classified in both): 1.00\n",
      "Model after Task 1 vs Model 2 evaluated on 1: Average SSIM Score (Forgotten): 0.07\n",
      "Model after Task 1 vs Model 2 evaluated on 1: Average SSIM Score (Correctly Classified in both): 0.14\n",
      "Model after Task 2 vs Model 2 evaluated on 2: No forgotten samples\n",
      "Model after Task 2 vs Model 2 evaluated on 2: Average SSIM Score (Correctly Classified in both): 1.00\n",
      "Model after Task 1 vs Model 3 evaluated on 1: Average SSIM Score (Forgotten): 0.03\n",
      "Model after Task 1 vs Model 3 evaluated on 1: Average SSIM Score (Correctly Classified in both): 0.07\n",
      "Model after Task 2 vs Model 3 evaluated on 2: Average SSIM Score (Forgotten): 0.06\n",
      "Model after Task 2 vs Model 3 evaluated on 2: Average SSIM Score (Correctly Classified in both): 0.08\n",
      "Model after Task 3 vs Model 3 evaluated on 3: No forgotten samples\n",
      "Model after Task 3 vs Model 3 evaluated on 3: Average SSIM Score (Correctly Classified in both): 1.00\n"
     ]
    }
   ],
   "source": [
    "NR_EPOCHS = 8\n",
    "BUFFER_SIZE = 3000\n",
    "model_final = SimpleVGG(num_classes=10)\n",
    "load_model(model_final, 2, NR_EPOCHS, BUFFER_SIZE)\n",
    "\n",
    "print(\"In the following we will compute the SSIM score between the model after each task T and the final model after task 3. The inputs used will correspond to the test set of task T.\")\n",
    "print(f\"Buffer Size: {BUFFER_SIZE}\")\n",
    "\n",
    "for final_model_idx in range(3):\n",
    "    model_final = SimpleVGG(num_classes=10)\n",
    "    load_model(model_final, final_model_idx, NR_EPOCHS, BUFFER_SIZE)\n",
    "    for task_idx in range(final_model_idx + 1):\n",
    "        model = SimpleVGG(num_classes=10)\n",
    "        load_model(model, task_idx, NR_EPOCHS, BUFFER_SIZE)\n",
    "        ssim_score, num_samples = compute_total_divergence_misclassified(model, model_final, test_subsets[task_idx])\n",
    "        if num_samples == 0:\n",
    "            print(f\"Model after Task {task_idx+1} vs Model {final_model_idx + 1} evaluated on {task_idx + 1}: No forgotten samples\")\n",
    "        else:\n",
    "            print(f\"Model after Task {task_idx+1} vs Model {final_model_idx + 1} evaluated on {task_idx + 1}: Average SSIM Score (Forgotten): {ssim_score/num_samples:.2f}\")\n",
    "        ssim_score, num_samples = compute_total_divergence_correctly_classified(model, model_final, test_subsets[task_idx])\n",
    "        if num_samples == 0:\n",
    "            print(f\"Model after Task {task_idx+1} vs Model {final_model_idx + 1} evaluated on {task_idx + 1}: No correctly classified samples\")\n",
    "        else:\n",
    "            print(f\"Model after Task {task_idx+1} vs Model {final_model_idx + 1} evaluated on {task_idx + 1}: Average SSIM Score (Correctly Classified in both): {ssim_score/num_samples:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the following we will compute the SSIM score between the model after each task T and the final model after task 3. The inputs used will correspond to the test set of task T.\n",
      "Buffer Size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22389/1272262104.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"task_{'>'.join(map(str, range(1, task_idx+2)))}_nr_epochs_{num_epochs}_buffersize_{buffer_size}.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after Task 1 vs Model 1 evaluated on 1: No forgotten samples\n",
      "Model after Task 1 vs Model 1 evaluated on 1: Average SSIM Score (Correctly Classified in both): 1.00\n",
      "Model after Task 1 vs Model 2 evaluated on 1: Average SSIM Score (Forgotten): 0.09\n",
      "Model after Task 1 vs Model 2 evaluated on 1: Average SSIM Score (Correctly Classified in both): 0.14\n",
      "Model after Task 2 vs Model 2 evaluated on 2: No forgotten samples\n",
      "Model after Task 2 vs Model 2 evaluated on 2: Average SSIM Score (Correctly Classified in both): 1.00\n",
      "Model after Task 1 vs Model 3 evaluated on 1: Average SSIM Score (Forgotten): 0.05\n",
      "Model after Task 1 vs Model 3 evaluated on 1: Average SSIM Score (Correctly Classified in both): 0.09\n",
      "Model after Task 2 vs Model 3 evaluated on 2: Average SSIM Score (Forgotten): 0.04\n",
      "Model after Task 2 vs Model 3 evaluated on 2: Average SSIM Score (Correctly Classified in both): 0.07\n",
      "Model after Task 3 vs Model 3 evaluated on 3: No forgotten samples\n",
      "Model after Task 3 vs Model 3 evaluated on 3: Average SSIM Score (Correctly Classified in both): 1.00\n"
     ]
    }
   ],
   "source": [
    "NR_EPOCHS = 8\n",
    "BUFFER_SIZE = 5000\n",
    "model_final = SimpleVGG(num_classes=10)\n",
    "load_model(model_final, 2, NR_EPOCHS, BUFFER_SIZE)\n",
    "\n",
    "print(\"In the following we will compute the SSIM score between the model after each task T and the final model after task 3. The inputs used will correspond to the test set of task T.\")\n",
    "print(f\"Buffer Size: {BUFFER_SIZE}\")\n",
    "\n",
    "for final_model_idx in range(3):\n",
    "    model_final = SimpleVGG(num_classes=10)\n",
    "    load_model(model_final, final_model_idx, NR_EPOCHS, BUFFER_SIZE)\n",
    "    for task_idx in range(final_model_idx + 1):\n",
    "        model = SimpleVGG(num_classes=10)\n",
    "        load_model(model, task_idx, NR_EPOCHS, BUFFER_SIZE)\n",
    "        ssim_score, num_samples = compute_total_divergence_misclassified(model, model_final, test_subsets[task_idx])\n",
    "        if num_samples == 0:\n",
    "            print(f\"Model after Task {task_idx+1} vs Model {final_model_idx + 1} evaluated on {task_idx + 1}: No forgotten samples\")\n",
    "        else:\n",
    "            print(f\"Model after Task {task_idx+1} vs Model {final_model_idx + 1} evaluated on {task_idx + 1}: Average SSIM Score (Forgotten): {ssim_score/num_samples:.2f}\")\n",
    "        ssim_score, num_samples = compute_total_divergence_correctly_classified(model, model_final, test_subsets[task_idx])\n",
    "        if num_samples == 0:\n",
    "            print(f\"Model after Task {task_idx+1} vs Model {final_model_idx + 1} evaluated on {task_idx + 1}: No correctly classified samples\")\n",
    "        else:\n",
    "            print(f\"Model after Task {task_idx+1} vs Model {final_model_idx + 1} evaluated on {task_idx + 1}: Average SSIM Score (Correctly Classified in both): {ssim_score/num_samples:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22389/1272262104.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"task_{'>'.join(map(str, range(1, task_idx+2)))}_nr_epochs_{num_epochs}_buffersize_{buffer_size}.pth\"))\n",
      "/home/alex/Documents/repos/ethz-deep-learning-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "/home/alex/Documents/repos/ethz-deep-learning-2024/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for final_model_idx in range(3):\n",
    "    model_final = SimpleVGG(num_classes=10)\n",
    "    load_model(model_final, final_model_idx, NR_EPOCHS, BUFFER_SIZE)\n",
    "    for task_idx in range(final_model_idx + 1):\n",
    "        model = SimpleVGG(num_classes=10)\n",
    "        load_model(model, task_idx, NR_EPOCHS, BUFFER_SIZE)\n",
    "        folder_path = f\"./deliverables/model_{final_model_idx+1}-->-model-task-{task_idx+1}-buffersize-{BUFFER_SIZE}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        with open(os.path.join(folder_path, \"info.txt\"), \"w\") as f:\n",
    "            f.write(f\"Visualizing the saliency map of 10 examples of forgotten samples (that were known in model_{task_idx+1}, but forgotten in model_{final_model_idx+1}) evaluated on task {task_idx+1}.\\n\")\n",
    "            f.write(f\"Visualizing the saliency map of 10 examples of remembered samples (that were known in both model_{task_idx+1} and model_{final_model_idx+1}) evaluated on task {task_idx+1}.\\n\")\n",
    "        visualize_misclassified_samples(\n",
    "            model, model_final, test_subsets[task_idx], num_samples=10, output_folder=folder_path\n",
    "        )\n",
    "        visualize_correctly_classified_samples(\n",
    "            model, model_final, test_subsets[task_idx], num_samples=10, output_folder=folder_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
